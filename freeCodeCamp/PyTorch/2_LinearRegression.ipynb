{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: freeCodeCamp - Full Course\n",
    "### Section 2: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70],\n",
    "                    [81, 101],\n",
    "                    [119, 133],\n",
    "                    [22, 37],\n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights and biases (randomly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3475, -0.0708,  0.7370],\n",
       "         [ 0.4653, -0.9704,  1.0436]], requires_grad=True),\n",
       " tensor([-1.0877, -0.4828], requires_grad=True))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run model on input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -72.5049,   13.3403],\n",
       "        [ -82.7688,   23.2527],\n",
       "        [ -85.0597,  -29.5078],\n",
       "        [-114.3032,   43.8601],\n",
       "        [ -49.2693,   11.5158]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MSE between model outputs and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17400.2246, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "loss = mse(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer gradient of loss with respect to parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-13289.3037, -14168.8750,  -8708.8887],\n",
       "         [ -6413.2046,  -8487.5010,  -4821.5435]]),\n",
       " tensor([-156.9812,  -79.5078]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "w.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a single gradient update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():   # no gradient tracking\n",
    "    w -= w.grad * 1e-5  # learning rate\n",
    "    b -= b.grad * 1e-5  # learning rate\n",
    "    w.grad.zero_()      # zero gradients\n",
    "    b.grad.zero_()      # zero gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 1000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4412,  0.7672,  0.8997],\n",
      "        [-0.3461,  0.7104,  1.1215]], requires_grad=True)\n",
      "tensor([-1.0798, -0.4890], requires_grad=True)\n",
      "tensor(5.3296, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(w)\n",
    "print(b)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would not manipulate the data this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43],[91, 88, 64],[87, 134, 58],[102, 43, 37],[69, 96, 70],[74, 66, 43],[91, 87, 65],[88, 134, 59],[101, 44, 37],[68, 96, 71],[73, 66, 44],[92, 87, 64],[87, 135, 57],[103, 43, 36],[68, 97, 70]], dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70],[81, 101],[119, 133],[22, 37],[103, 119],[57, 69],[80, 102],[118, 132],[21, 38],[104, 118],[57, 69],[82, 100],[118, 134],[20, 38],[102, 120]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a tensor dataset for additional useful functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in batches and with shuffling using the data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 68.,  96.,  71.],\n",
      "        [ 87., 135.,  57.]])\n",
      "tensor([[104., 118.],\n",
      "        [118., 134.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 2\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initialising weights and biases manually, we can do this automatically by defining a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1656,  0.2584, -0.1107],\n",
      "        [ 0.1520,  0.2651, -0.1415]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5334, 0.4427], requires_grad=True)\n",
      "[Parameter containing:\n",
      "tensor([[ 0.1656,  0.2584, -0.1107],\n",
      "        [ 0.1520,  0.2651, -0.1415]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5334, 0.4427], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter inputs and obtain predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.1753, 23.2163],\n",
       "        [31.2577, 28.5479],\n",
       "        [43.1465, 40.9830],\n",
       "        [24.4404, 22.1117],\n",
       "        [29.0172, 26.4752],\n",
       "        [25.0825, 23.1032],\n",
       "        [30.8885, 28.1413],\n",
       "        [43.2014, 40.9935],\n",
       "        [24.5332, 22.2247],\n",
       "        [28.7409, 26.1816],\n",
       "        [24.8061, 22.8097],\n",
       "        [31.1649, 28.4348],\n",
       "        [43.5156, 41.3896],\n",
       "        [24.7167, 22.4052],\n",
       "        [29.1100, 26.5882]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining the loss function manually, we can use built-in ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3929.1677, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "opt = SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 3.4481\n",
      "Epoch [20/100], Loss: 30.5775\n",
      "Epoch [30/100], Loss: 1.4171\n",
      "Epoch [40/100], Loss: 7.7226\n",
      "Epoch [50/100], Loss: 1.1432\n",
      "Epoch [60/100], Loss: 9.1919\n",
      "Epoch [70/100], Loss: 0.9749\n",
      "Epoch [80/100], Loss: 0.7797\n",
      "Epoch [90/100], Loss: 2.4805\n",
      "Epoch [100/100], Loss: 3.8318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2207, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    for epoch in range(num_epochs):     # For each epoch\n",
    "        for xb, yb in train_dl:         # For each batch\n",
    "            pred = model(xb)            # Make predictions\n",
    "            loss = loss_fn(pred, yb)    # Calculate loss\n",
    "            loss.backward()             # Compute loss gradients\n",
    "            opt.step()                  # Update model parameters \n",
    "            opt.zero_grad()             # Reset gradients\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "fit(100, model, loss_fn, opt, train_dl)\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predictions with targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4922,  0.8055],\n",
       "        [ 0.5988, -1.3429],\n",
       "        [ 1.5285,  2.4083],\n",
       "        [-0.3947,  1.5849],\n",
       "        [-2.4087, -2.5212],\n",
       "        [-0.7777,  0.7006],\n",
       "        [ 1.2939, -2.4552],\n",
       "        [ 2.7327,  3.8944],\n",
       "        [ 1.8752,  1.6898],\n",
       "        [-2.4437, -0.5286],\n",
       "        [ 0.1873,  1.6933],\n",
       "        [-1.6711, -1.4479],\n",
       "        [ 2.8334,  1.5205],\n",
       "        [ 0.6403, -0.4078],\n",
       "        [-0.1388, -2.4163]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs) - targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show documentation using the question mark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "Args:\n",
      "    in_features: size of each input sample\n",
      "    out_features: size of each output sample\n",
      "    bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "        Default: ``True``\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      "      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      "    - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      "      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "Attributes:\n",
      "    weight: the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "        :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "            If :attr:`bias` is ``True``, the values are initialized from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> m = nn.Linear(20, 30)\n",
      "    >>> input = torch.randn(128, 20)\n",
      "    >>> output = m(input)\n",
      "    >>> print(output.size())\n",
      "    torch.Size([128, 30])\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     NonDynamicallyQuantizableLinear, LazyLinear, Linear, LinearBn1d, Linear"
     ]
    }
   ],
   "source": [
    "?nn.Linear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
